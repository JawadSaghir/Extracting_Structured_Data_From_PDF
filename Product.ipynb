{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -qU langgraph langchain_google-genai langchain_community\n",
    "!pip3 install --upgrade --quiet pypdf pandas==2.2.2 streamlit python-dotenv\n",
    "!pip3 install --quiet --upgrade langchain\n",
    "!pip3 install -qU langchain-google-genai\n",
    "!pip3 install --upgrade --quiet  langchain-huggingface sentence_transformers\n",
    "!pip3 install -qU chromadb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,getpass\n",
    "key=os.environ[\"Gemini-2.0-flash\"]=getpass.getpass(\"Enter You API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "import tempfile\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\",\n",
    "                       api_key=key,\n",
    "                       temperature=0.4,\n",
    "                       top_p=0.4\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader=PyPDFLoader(\"/content/IJAMR2304261.pdf\")\n",
    "pages=loader.load()\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,\n",
    "                                             chunk_overlap=200,\n",
    "                                             separators=[\"\\n\\n\",\"\\n\",\" \"])\n",
    "chunks=text_splitter.split_documents(pages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_function():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "    # Convert the list of Documents to a list of strings before embedding\n",
    "    return embeddings\n",
    "embedding_function=embedding_function()\n",
    "texts = [doc.page_content for doc in chunks]\n",
    "query_result = embedding_function.embed_documents(texts)  # Use embed_documents for multiple texts\n",
    "    # If you need a single embedding for all the text, you can combine them first\n",
    "    # all_text = \" \".join(texts)\n",
    "    # query_result = embeddings.embed_query(all_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How embedding works\n",
    "from langchain.evaluation import load_evaluator\n",
    "evaluator = load_evaluator(evaluator=\"embedding_distance\",\n",
    "                           embeddings=embeddings) # Pass the instance to load_evaluator\n",
    "evaluator.evaluate_strings(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "def create_vectorstore(chunks,embedding_function,vectorstore_path):\n",
    "  # Changed 'docs' to 'chunks' to iterate through the provided chunks variable\n",
    "  ids=[str(uuid.uuid5(uuid.NAMESPACE_DNS,doc.page_content))for doc in chunks]\n",
    "  unique_ids=set()\n",
    "  unique_chunks=[]\n",
    "  for chunk,id in zip(chunks,ids):\n",
    "    if id not in unique_ids:\n",
    "      unique_ids.add(id)\n",
    "      unique_chunks.append(chunk)\n",
    "  vectorstore=Chroma.from_documents(documents=chunks,\n",
    "                                  embedding=embedding_function,\n",
    "                                  persist_directory=\"vectorstore\")\n",
    "  vectorstore.persist()\n",
    "\n",
    "  return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore=create_vectorstore(chunks=chunks,\n",
    "                               embedding_function=embedding_function,\n",
    "                               vectorstore_path=\"vectorstore_chroma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore=Chroma(persist_directory=\"vectorstore_choma\",\n",
    "                   embedding_function=embedding_function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=vectorstore.as_retriever(search_type=\"similarity\")\n",
    "relevant_chunks=retriever.invoke(\"What is the title of the article? \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# template=ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         ('system','You are an assistant for question-answering tasks.Use the following pieces of retrieved context to answer the question.if you donot know the answer,just say that you donot know,dont try to make up an answer.'),\n",
    "#         (\"human\",\"Answer the question precisely based on the above context : {question}\")\n",
    "#     ]\n",
    "\n",
    "# )\n",
    "\n",
    "Prompt_Template=\"\"\"\n",
    "You are an assistant for question-answering tasks.Use the following pieces of retrieved context to answer the question.if you donot know the answer,just say that you donot know,dont try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "Answer the question precisely based on the above context : {question}\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_text=\"\\n\\n--\\n\\n\".join([doc.page_content for doc in relevant_chunks])\n",
    "prompt_template=ChatPromptTemplate.from_template(template=Prompt_Template)\n",
    "prompt= prompt_template.format(context=context_text,\n",
    "                        question=\"What is the title of the article?\")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
